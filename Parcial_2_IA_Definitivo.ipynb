{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhct9e_7LH0C"
      },
      "source": [
        "# **Parcial 2: Prediciendo el PIB por medio de redes neuronales**\n",
        "\n",
        "**Universidad de los Andes**\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "*   Marcelo Yepes\n",
        "\n",
        "*   Angela Sofia Torres\n",
        "\n",
        "*   Jennifer Paola Sarabia\n",
        "\n",
        "*  Laura Pabón\n",
        "\n",
        "*   Dario Montoya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phj3H7QeL4bf"
      },
      "source": [
        "# **Instalación de Bibliotecas y Carga del Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km5jwF77LUu6"
      },
      "outputs": [],
      "source": [
        "#Instalación de las librerías\n",
        "!pip install -q pandas\n",
        "!pip install -q numpy\n",
        "!pip install -q scipy\n",
        "!pip install -q matplotlib\n",
        "!pip install -q seaborn\n",
        "!pip install -q plotly\n",
        "!pip install -q yellowbrick\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q imbalanced-learn\n",
        "!pip install -q tqdm\n",
        "!pip install -q joblib\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q datasets\n",
        "!pip install -q ydata_profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftw08dZkMPz4"
      },
      "source": [
        "**Librerías de Machine Learning**\n",
        "\n",
        "*   ColumnTransformer: Para aplicar diferentes transformaciones a diferentes columnas.\n",
        "*  train_test_split: Divide los datos en entrenamiento y prueba.\n",
        "*   Escaladores (MinMaxScaler, StandardScaler): Normalización de datos.\n",
        "*   Codificadores (LabelEncoder, OneHotEncoder): Convierte datos categóricos en numéricos.\n",
        "*   Métricas: Evalúan el rendimiento de los modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORChpvA-MMZG"
      },
      "outputs": [],
      "source": [
        "#Importar las librerías necesarias y definir semilla\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from google.colab import files\n",
        "from ydata_profiling import ProfileReport\n",
        "from numpy import sqrt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, FunctionTransformer, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Quitar el límite de columnas a mostrar en un DataFrame.\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Fijar semilla para reproducibilidad\n",
        "SEED = 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbdzWuMDM__z"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download sazidthe1/world-gdp-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjZKH3esPq3v"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'world-gdp-data'\n",
        "!unzip -o {DATASET_NAME}.zip -d {DATASET_NAME}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFRRrPgN_ACH"
      },
      "source": [
        "# **1. Preparación y procesamiento de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg7aerg7TRG1"
      },
      "source": [
        "## **Juntar las bases de datos de acuerdo con el código del país**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJf6urstROBV"
      },
      "outputs": [],
      "source": [
        "# Listar archivos en el directorio actual\n",
        "print(os.listdir(\"world-gdp-data\"))\n",
        "\n",
        "# Cargar los datasets en formato csv\n",
        "gdp_df = pd.read_csv(\"/content/world-gdp-data/gdp_data.csv\")\n",
        "country_df = pd.read_csv(\"/content/world-gdp-data/country_codes.csv\")\n",
        "\n",
        "\n",
        "# Verificar contenido antes de la unión\n",
        "print(\"GDP Dataset:\")\n",
        "display(gdp_df.head())\n",
        "\n",
        "print(\"\\nCountry Codes Dataset:\")\n",
        "display(country_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7AwhA90PoLD"
      },
      "outputs": [],
      "source": [
        "# Unir los datasets usando la columna 'country_code'\n",
        "df_merged= pd.merge(gdp_df, country_df, on=\"country_code\", how=\"inner\")\n",
        "\n",
        "df_merged = df_merged.rename(columns={\"value\": \"gdp\"})\n",
        "\n",
        "# Mostrar las filas del dataset combinado\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bono: añadir una nueva variable"
      ],
      "metadata": {
        "id": "f_Tpnyg5HHN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tomó la variable de porcentaje de inflación anual para 266 países con datos desde 1960 hasta 2022. Esta base de datos se tomó del Banco Mundial y se encuentra en este enlace: https://data.worldbank.org/indicator/FP.CPI.TOTL.ZG?view=chart\n",
        "\n",
        "Se escogió tomar la variable de inflación ya que se cree que niveles altos de inflación tiene una relación negativa sobre el PIB. La inlación indica los aumentos generalizados de precios, lo cual reduce el poder adquisitivo y puede contraer el consumo y la inversión, componentes cruciales del PIB. Así una inflación alta y volátil puede desacelerar el crecimiento económico, afectando negativamente la competitividad y el comercio exterior de un país, lo que se puede asociar con un menor PIB. Por este motivo, consideramos que la variables de inflación puede ser útil para predecir el nivel del PIB de cada país."
      ],
      "metadata": {
        "id": "V6liNiZ5HL62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar la base de datos de inflación manualmente\n",
        "df_inflacion = pd.read_csv(\"/content/inflacion_data.csv\", sep=\",\", skiprows=3)\n",
        "\n",
        "df_inflacion.info()\n",
        "\n",
        "df_inflacion = df_inflacion.drop(columns=['Country Name', 'Indicator Name', 'Indicator Code', '2023', 'Unnamed: 68'])\n",
        "\n",
        "df_inflacion"
      ],
      "metadata": {
        "id": "DTZJcTH6H3fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertir los años en filas para que quede en el mismo formato que gdp_df\n",
        "\n",
        "df_inflacion_long = df_inflacion.melt(id_vars=[\"Country Code\"], var_name=\"year\", value_name=\"inflation\")\n",
        "df_inflacion_long[\"year\"] = df_inflacion_long[\"year\"].astype(int)\n",
        "df_inflacion_long = df_inflacion_long.rename(columns={\"Country Code\": \"country_code\"})\n",
        "\n",
        "df_inflacion_long"
      ],
      "metadata": {
        "id": "jaYmnCBFIWMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unir el merge realizado anteriormente con los datos del PIB con la base de datos de inflación usando el código dle país\n",
        "df = df_merged.merge(df_inflacion_long, on=[\"country_code\", \"year\"], how=\"right\")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "dMlN-F7-Im_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlTSp3HDe4Lq"
      },
      "source": [
        "##**Explorar el dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX_9Ft_rfHPo"
      },
      "source": [
        "**Diccionario de variables**\n",
        "\n",
        "| **Variable**   | **Descripción**                                                                                  | **Tipo de dato** |\n",
        "|---------------|--------------------------------------------------------------------------------------------------|------------------|\n",
        "| country_name  | Nombre del país.                                                                                 | Texto           |\n",
        "| country_code  | Código del país según la norma (código de tres letras).                        | Texto           |\n",
        "| year          | Año al que corresponde el dato.                                                          | Numérico        |\n",
        "| value         | Valor del PIB en dólares estadounidenses actuales.                                               | Numérico        |\n",
        "| region        | Región geográfica a la que pertenece el país (por ejemplo, Asia, Europa).                        | Texto           |\n",
        "| income_group  | Clasificación del país según su nivel de ingresos (por ejemplo, ingresos altos, medios, bajos). | Texto           |\n",
        "| inflation     | Tasa de inflación anual (%), basada en el índice de precios al consumidor.                        | Numérico        |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVR1r_g6TrCr"
      },
      "outputs": [],
      "source": [
        "# Información general de los datasets\n",
        "print(gdp_df.info())\n",
        "print(country_df.info())\n",
        "print(df_inflacion.info())\n",
        "\n",
        "# Estadísticas descriptivas\n",
        "print (\"Estadísticas descriptivas de la base de datos\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20Oteu9QdTR"
      },
      "source": [
        "## **Clasificación de variables y preparación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "123RLepV68Jd"
      },
      "outputs": [],
      "source": [
        "# Hacer Label Encoder para la variable 'income_group'\n",
        "\n",
        "# Crear un diccionario con los valores personalizados\n",
        "income_mapping = {\n",
        "    \"High income\": 4,\n",
        "    \"Upper middle income\": 3,\n",
        "    \"Lower middle income\": 2,\n",
        "    \"Low income\": 1\n",
        "}\n",
        "\n",
        "# Aplicar el mapeo a la columna 'income_group'\n",
        "df['income_group'] = df['income_group'].map(income_mapping)\n",
        "\n",
        "# Aplicar One-Hot Encoding a 'region'\n",
        "df_encoded = pd.get_dummies(df, columns=['region'])\n",
        "df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBzf5QX-hhcg"
      },
      "outputs": [],
      "source": [
        "# Para transformar la variable categórica de los países\n",
        "\n",
        "# Modificar el nombre de los paises por un ranking\n",
        "country_gpd = df_encoded.groupby('country_name')['gdp'].mean().sort_values()\n",
        "country_ranking = {country: rank for rank, country in enumerate(country_gpd.index)}\n",
        "\n",
        "df_encoded['country_name'] = df_encoded['country_name'].map(country_ranking)\n",
        "\n",
        "df_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF3Z9sb_HPDF"
      },
      "outputs": [],
      "source": [
        "#Valores únicos en la variable año\n",
        "df_encoded['year'].unique()\n",
        "\n",
        "#Transformación de los años en columnas\n",
        "\n",
        "df_final = df.pivot(index=\"country_code\", columns=\"year\", values=[\"gdp\", \"inflation\"])\n",
        "\n",
        "df_final.columns[-64]\n",
        "\n",
        "df_final\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificar valor NA en la base de datos\n",
        "print(df_final.isna().sum())\n",
        "\n",
        "# Reemplazar todos los NaN en el DataFrame con -1\n",
        "df_final.fillna(-1, inplace=True)\n",
        "\n",
        "print(df_final.isna().sum())"
      ],
      "metadata": {
        "id": "Ip-NWWQ_KnIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación de valores nulos\n",
        "print(df_final.isnull().sum())"
      ],
      "metadata": {
        "id": "eXC03zOuL9NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHP4Kcs7_dX1"
      },
      "source": [
        "**Crear la variable objetivo (GDP para el año 2022)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.columns[-64]\n"
      ],
      "metadata": {
        "id": "Nlr7-YP0MV0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2AuLUR0W1RP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Definir el nombre de la columna correspondiente al PIB del 2022\n",
        "gdp_2022 = ('gdp', 2022)\n",
        "gdp_2021 = ('gdp', 2021)\n",
        "\n",
        "# Reemplazar NaN en la columna 2022 con los valores de la columna 2021\n",
        "df_final[gdp_2022] = df_final[gdp_2022].fillna(df_final[gdp_2021])\n",
        "\n",
        "# Calcular percentiles SOLO sobre la columna 2022\n",
        "low_threshold = np.percentile(df_final[gdp_2022].dropna(), 33)  # 33% de los datos\n",
        "high_threshold = np.percentile(df_final[gdp_2022].dropna(), 66)  # 66% de los datos\n",
        "\n",
        "# Función para asignar la categoría basada en los percentiles de 2022\n",
        "def categorize_gdp(value):\n",
        "    if value <= low_threshold:\n",
        "        return \"Low GDP\"\n",
        "    elif value <= high_threshold:\n",
        "        return \"Medium GDP\"\n",
        "    else:\n",
        "        return \"High GDP\"\n",
        "\n",
        "# Aplicar la función de categorización sobre la columna GDP_2022\n",
        "df_final[\"GDP_Level\"] = df_final[gdp_2022].apply(categorize_gdp)\n",
        "\n",
        "# Mostrar la distribución de clases\n",
        "print(df_final[\"GDP_Level\"].value_counts())\n",
        "\n",
        "# Codificar la variable categórica con LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_final[\"GDP_Level\"] = le.fit_transform(df_final[\"GDP_Level\"])\n",
        "\n",
        "df_final.drop(columns=[gdp_2022], inplace=True)\n",
        "\n",
        "# Mostrar primeras filas con la variable de clasificación\n",
        "display(df_final.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnOCA0QC24G1"
      },
      "source": [
        "## **División de los datos en test y train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qejn75iZgxx"
      },
      "outputs": [],
      "source": [
        "# Separar características (X) y variable objetivo (y)\n",
        "X = df_final.drop(columns=[\"GDP_Level\"])\n",
        "y = df_final[\"GDP_Level\"]\n",
        "\n",
        "# Dividir en 80% entrenamiento y 20% prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Mostrar dimensiones de los conjuntos\n",
        "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_wfMiW649B-"
      },
      "outputs": [],
      "source": [
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEbNjdiL3BwM"
      },
      "source": [
        "### **Análisis descriptivo de los datos de train y test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cgv1_p-3HR-"
      },
      "source": [
        "#### **Análisis datos train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqukum-D3ccD"
      },
      "outputs": [],
      "source": [
        "# Reporte análisis descriptivo de los datos de entrenamiento\n",
        "reporte_train = ProfileReport(df_train, title=\"Profiling Report Train dataset\", minimal=True)\n",
        "reporte_train.to_file(\"reporte_train.html\")\n",
        "reporte_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEN9PUQO5ctE"
      },
      "source": [
        "#### **Análisis datos test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQlAwRm65j8x"
      },
      "outputs": [],
      "source": [
        "# Reporte análisis descriptivo de los datos de entrenamiento\n",
        "reporte_test = ProfileReport(df_train, title=\"Profiling Report Test dataset\", minimal=True)\n",
        "reporte_test.to_file(\"reporte_test.html\")\n",
        "reporte_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p8arN81ZlU5"
      },
      "source": [
        "# **2. Construcción modelos redes neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. a. Modelo de red neuronal tradicional (Scikit-Learn)"
      ],
      "metadata": {
        "id": "s3wCXBBINlAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmilR9gNZxko"
      },
      "outputs": [],
      "source": [
        "#Estandarizar los datos de X de la base detos\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# 📌 Guardar nombres de columnas y el índice antes de escalar\n",
        "column_names = X_train.columns\n",
        "index_train = X_train.index\n",
        "index_test = X_test.index\n",
        "\n",
        "# 📌 Escalar los datos después de la limpieza\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 📌 Convertir de nuevo a DataFrame manteniendo nombres de columnas e índice\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=column_names, index=index_train)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=column_names, index=index_test)\n",
        "\n",
        "display(X_train_scaled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Búsqueda de hiperparámetros para modelo tradicional"
      ],
      "metadata": {
        "id": "g2ZusNjTOCF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Definir los hiperparámetros a optimizar con solo una capa oculta\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(16,), (32,), (64,), (128,)],  # Diferentes tamaños para una única capa oculta\n",
        "    'activation': ['relu', 'tanh', 'logistic'],  # Función de activación\n",
        "    'solver': ['adam', 'lbfgs'],  # Métodos de optimización\n",
        "    'alpha': [0.0001, 0.01, 0.1],  # Regularización L2\n",
        "    'learning_rate_init': [0.001, 0.01, 0.05],  # Tasa de aprendizaje inicial\n",
        "    'batch_size': [32, 64, 'auto'],  # Tamaño del lote\n",
        "}\n",
        "\n",
        "# Definir el modelo base\n",
        "mlp = MLPClassifier(max_iter=200, random_state=42)\n",
        "\n",
        "# Configurar Grid Search\n",
        "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Ejecutar la búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Mostrar los mejores hiperparámetros encontrados\n",
        "print(\"\\n🔍 Mejores hiperparámetros encontrados:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Entrenar modelo con los mejores hiperparámetros\n",
        "best_mlp = grid_search.best_estimator_\n",
        "\n",
        "# Evaluar en el conjunto de prueba\n",
        "y_pred = best_mlp.predict(X_test_scaled)\n",
        "print(\"\\n✅ Resultados del mejor modelo:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PacFUhJNeTXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K8Tw3jPaTAc"
      },
      "outputs": [],
      "source": [
        "#  PARA TEST\n",
        "#Paso 1: Hacer predicciones\n",
        "y_pred = best_mlp.predict(X_test_scaled)\n",
        "\n",
        "#  Paso 2: Imprimir reporte de clasificación\n",
        "print(\"\\n✅ Reporte de Clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#  Paso 3: Graficar la Matriz de Confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusión - MLPClassifier Test\")\n",
        "plt.show()\n",
        "\n",
        "#  PARA TRAIN\n",
        "#Paso 1: Hacer predicciones\n",
        "y_pred = best_mlp.predict(X_train_scaled)\n",
        "\n",
        "#  Paso 2: Imprimir reporte de clasificación\n",
        "print(\"\\n✅ Reporte de Clasificación:\")\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "#  Paso 3: Graficar la Matriz de Confusión\n",
        "conf_matrix = confusion_matrix(y_train, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusión - MLPClassifier Train\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfica función de pérdida por iteración\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 📌 Verificar si el modelo ha almacenado el historial de pérdidas\n",
        "if hasattr(best_mlp, 'loss_curve_'):\n",
        "    plt.plot(best_mlp.loss_curve_, label='Loss durante entrenamiento')\n",
        "    plt.xlabel('Iteraciones (Épocas)')\n",
        "    plt.ylabel('Pérdida (Loss)')\n",
        "    plt.title('Evolución de la Pérdida durante el Entrenamiento')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"El modelo no ha almacenado la curva de pérdida.\")\n",
        "\n",
        "#Gráfica tasa de aprendizaje\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 📌 Definir la tasa de aprendizaje inicial (se obtiene de los hiperparámetros del mejor modelo)\n",
        "learning_rate_init = best_mlp.learning_rate_init\n",
        "\n",
        "# 📌 Simular la evolución de la tasa de aprendizaje si es 'adaptive'\n",
        "if best_mlp.learning_rate == 'adaptive':\n",
        "    learning_rates = [learning_rate_init / (1.0 + 0.1 * i) for i in range(len(best_mlp.loss_curve_))]\n",
        "else:\n",
        "    learning_rates = [learning_rate_init] * len(best_mlp.loss_curve_)  # Tasa de aprendizaje fija\n",
        "\n",
        "# Graficar la tasa de aprendizaje\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(learning_rates, label=\"Tasa de Aprendizaje\", color='red')\n",
        "plt.xlabel('Iteraciones (Épocas)')\n",
        "plt.ylabel('Tasa de Aprendizaje')\n",
        "plt.title('Evolución de la Tasa de Aprendizaje durante el Entrenamiento')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#Curva ROC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 📌 Binarizar las clases de y_test para el cálculo de la ROC multiclase\n",
        "n_classes = len(np.unique(y_test))  # Número de clases únicas\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))  # Convierte en formato binarizado\n",
        "y_probs = best_mlp.predict_proba(X_test_scaled)  # Probabilidades de cada clase\n",
        "\n",
        "# 📌 Calcular ROC para cada clase\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_probs[:, i])  # ROC para cada clase\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'Clase {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "# 📌 Agregar línea de referencia y etiquetas\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC Multiclase')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Gráfica de precisión"
      ],
      "metadata": {
        "id": "qNVi-XOOTa-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfica precisión del modelo"
      ],
      "metadata": {
        "id": "1z9sihd7W0Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 📌 Verificar si el modelo tiene historial de entrenamiento\n",
        "if hasattr(best_mlp, \"loss_curve_\"):\n",
        "    plt.plot(best_mlp.loss_curve_, label=\"Pérdida en entrenamiento\", color=\"b\")\n",
        "\n",
        "    plt.xlabel(\"Iteraciones (Épocas)\")\n",
        "    plt.ylabel(\"Pérdida (Loss)\")\n",
        "    plt.title(\"Evolución de la Pérdida del Modelo\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ No hay historial de pérdida disponible. Verifica que el modelo haya sido entrenado correctamente.\")\n"
      ],
      "metadata": {
        "id": "3ccp2-XbWcZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para guardar el modelo\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Guardar modelo de Scikit-Learn\n",
        "joblib.dump(best_mlp, \"modelo_sklearn.pkl\")\n",
        "\n",
        "# Cargar modelo después\n",
        "modelo_sklearn = joblib.load(\"modelo_sklearn.pkl\")\n"
      ],
      "metadata": {
        "id": "NAJkrhXNclEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. b. Modelo red neuronal profunda (Tensorflow)"
      ],
      "metadata": {
        "id": "3akexs-lhu5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6wj7yG3auSH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 📌 Paso 1: Definir la arquitectura de la Red Neuronal Profunda\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Capa de entrada\n",
        "    Dropout(0.2),  # Regularización\n",
        "    Dense(64, activation='relu'),  # Primera capa oculta\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),  # Segunda capa oculta\n",
        "    Dense(3, activation='softmax')  # Capa de salida (3 clases: Low, Medium, High GDP)\n",
        "])\n",
        "\n",
        "# 📌 Paso 2: Compilar el modelo\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),  # Optimizador Adam\n",
        "              loss='sparse_categorical_crossentropy',  # Función de pérdida\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 📌 Paso 3: Entrenar el modelo\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# 📌 Paso 4: Evaluación final\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"\\n📊 Precisión en el conjunto de prueba: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meTWY9R6a9hU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#PARA TEST\n",
        "# 📌 Hacer predicciones\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "y_pred_nn = y_pred_nn.argmax(axis=1)  # Convertir probabilidades en clases\n",
        "\n",
        "# 📌 Imprimir reporte de clasificación\n",
        "print(\"\\n✅ Reporte de Clasificación - Red Neuronal TensorFlow:\")\n",
        "print(classification_report(y_test, y_pred_nn))\n",
        "\n",
        "# 📌 Matriz de Confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_nn)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusión - TensorFlow Test\")\n",
        "plt.show()\n",
        "\n",
        "#PARA TRAIN\n",
        "# 📌 Hacer predicciones\n",
        "y_pred_nn = model.predict(X_train_scaled)\n",
        "y_pred_nn = y_pred_nn.argmax(axis=1)  # Convertir probabilidades en clases\n",
        "\n",
        "# 📌 Imprimir reporte de clasificación\n",
        "print(\"\\n✅ Reporte de Clasificación - Red Neuronal TensorFlow:\")\n",
        "print(classification_report(y_train, y_pred_nn))\n",
        "\n",
        "# 📌 Matriz de Confusión\n",
        "conf_matrix = confusion_matrix(y_train, y_pred_nn)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusión - TensorFlow Train\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXmXWzMCa5bT"
      },
      "outputs": [],
      "source": [
        "#Gráfica tasa de aprendizaje\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extraer la tasa de aprendizaje de cada época desde el optimizador\n",
        "learning_rates = [model.optimizer.learning_rate.numpy() for _ in history.epoch]\n",
        "\n",
        "# Graficar la evolución de la tasa de aprendizaje\n",
        "plt.plot(history.epoch, learning_rates, label=\"Tasa de Aprendizaje\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Evolución de la Tasa de Aprendizaje\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Pérdida del modelo\n",
        "plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Validación')\n",
        "plt.title(\"Evolución de la función de pérdida modelo redes neuronales profundas\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Pérdida\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Precisión en entrenamiento y validación\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Validación')\n",
        "plt.title(\"Evolución de la precisión modelo redes neuronales profundas\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Precisión\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Suponiendo que entrenaste el modelo con history = modelo.fit(...)\n",
        "plt.plot(history.history['loss'], label='Pérdida en entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida en validación')\n",
        "\n",
        "plt.xlabel('Iteraciones (Épocas)')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.title('Tasa de Aprendizaje vs Iteraciones')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Para guardar el modelo\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# Guardar modelo de TensorFlow\n",
        "modelo_tf.save(\"modelo_tensorflow.keras\")\n",
        "\n",
        "# Cargar modelo después\n",
        "modelo_tensorflow = keras.models.load_model(\"modelo_tensorflow.keras\")"
      ],
      "metadata": {
        "id": "C-4KX9Hycvpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Red neuronal con mala función de pérdida"
      ],
      "metadata": {
        "id": "iBiggcXZiCeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j971aTGBbIUP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# 📌 Paso 1: Definir la arquitectura del modelo con errores\n",
        "model_bad = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Capa de entrada\n",
        "    Dense(64, activation='relu'),  # Primera capa oculta (sin Dropout)\n",
        "    Dense(32, activation='relu'),  # Segunda capa oculta (sin BatchNorm)\n",
        "    Dense(3, activation='softmax')  # Capa de salida (3 clases)\n",
        "])\n",
        "\n",
        "\n",
        "# 📌 Paso 2: Compilar con errores\n",
        "model_bad.compile(optimizer=SGD(learning_rate=1.0),  # Learning rate demasiado alto\n",
        "                  loss='sparse_categorical_crossentropy',  # Función de pérdida incorrecta (debería ser 'sparse_categorical_crossentropy')\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# 📌 Paso 3: Entrenar el modelo\n",
        "history_bad = model_bad.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# 📌 Paso 4: Evaluación del modelo\n",
        "test_loss_bad, test_acc_bad = model_bad.evaluate(X_test_scaled, y_test)\n",
        "print(f\"\\n❌ Precisión en test (modelo erróneo): {test_acc_bad:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INJNhHsLbQVI"
      },
      "outputs": [],
      "source": [
        "# 📌 Gráfica de la pérdida (se espera que sea inestable)\n",
        "plt.plot(history_bad.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history_bad.history['val_loss'], label='Validación')\n",
        "plt.title(\"🚨 Pérdida en el modelo mal configurado\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Pérdida\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 📌 Gráfica de la precisión (se espera que no mejore mucho)\n",
        "plt.plot(history_bad.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history_bad.history['val_accuracy'], label='Validación')\n",
        "plt.title(\"🚨 Precisión en el modelo mal configurado\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Precisión\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N7j-iDUbSJ9"
      },
      "outputs": [],
      "source": [
        "# 📌 Hacer predicciones con el modelo mal configurado\n",
        "y_pred_bad = model_bad.predict(X_test_scaled)\n",
        "y_pred_bad = y_pred_bad.argmax(axis=1)  # Convertir probabilidades en clases\n",
        "\n",
        "# 📌 Matriz de confusión\n",
        "conf_matrix_bad = confusion_matrix(y_test, y_pred_bad)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix_bad, annot=True, fmt='d', cmap='Reds', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"🚨 Matriz de Confusión - Modelo Mal Configurado\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
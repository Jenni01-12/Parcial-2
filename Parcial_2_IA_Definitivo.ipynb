{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhct9e_7LH0C"
      },
      "source": [
        "# **Parcial 2: Prediciendo el PIB por medio de redes neuronales**\n",
        "\n",
        "**Universidad de los Andes**\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "*   Marcelo Yepes\n",
        "\n",
        "*   Angela Sofia Torres\n",
        "\n",
        "*   Jennifer Paola Sarabia\n",
        "\n",
        "*  Laura Pab√≥n\n",
        "\n",
        "*   Dario Montoya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phj3H7QeL4bf"
      },
      "source": [
        "# **Instalaci√≥n de Bibliotecas y Carga del Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km5jwF77LUu6"
      },
      "outputs": [],
      "source": [
        "#Instalaci√≥n de las librer√≠as\n",
        "!pip install -q pandas\n",
        "!pip install -q numpy\n",
        "!pip install -q scipy\n",
        "!pip install -q matplotlib\n",
        "!pip install -q seaborn\n",
        "!pip install -q plotly\n",
        "!pip install -q yellowbrick\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q imbalanced-learn\n",
        "!pip install -q tqdm\n",
        "!pip install -q joblib\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q datasets\n",
        "!pip install -q ydata_profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftw08dZkMPz4"
      },
      "source": [
        "**Librer√≠as de Machine Learning**\n",
        "\n",
        "*   ColumnTransformer: Para aplicar diferentes transformaciones a diferentes columnas.\n",
        "*  train_test_split: Divide los datos en entrenamiento y prueba.\n",
        "*   Escaladores (MinMaxScaler, StandardScaler): Normalizaci√≥n de datos.\n",
        "*   Codificadores (LabelEncoder, OneHotEncoder): Convierte datos categ√≥ricos en num√©ricos.\n",
        "*   M√©tricas: Eval√∫an el rendimiento de los modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORChpvA-MMZG"
      },
      "outputs": [],
      "source": [
        "#Importar las librer√≠as necesarias y definir semilla\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from google.colab import files\n",
        "from ydata_profiling import ProfileReport\n",
        "from numpy import sqrt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, FunctionTransformer, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Quitar el l√≠mite de columnas a mostrar en un DataFrame.\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Fijar semilla para reproducibilidad\n",
        "SEED = 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbdzWuMDM__z"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download sazidthe1/world-gdp-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjZKH3esPq3v"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'world-gdp-data'\n",
        "!unzip -o {DATASET_NAME}.zip -d {DATASET_NAME}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFRRrPgN_ACH"
      },
      "source": [
        "# **1. Preparaci√≥n y procesamiento de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg7aerg7TRG1"
      },
      "source": [
        "## **Juntar las bases de datos de acuerdo con el c√≥digo del pa√≠s**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJf6urstROBV"
      },
      "outputs": [],
      "source": [
        "# Listar archivos en el directorio actual\n",
        "print(os.listdir(\"world-gdp-data\"))\n",
        "\n",
        "# Cargar los datasets en formato csv\n",
        "gdp_df = pd.read_csv(\"/content/world-gdp-data/gdp_data.csv\")\n",
        "country_df = pd.read_csv(\"/content/world-gdp-data/country_codes.csv\")\n",
        "\n",
        "\n",
        "# Verificar contenido antes de la uni√≥n\n",
        "print(\"GDP Dataset:\")\n",
        "display(gdp_df.head())\n",
        "\n",
        "print(\"\\nCountry Codes Dataset:\")\n",
        "display(country_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7AwhA90PoLD"
      },
      "outputs": [],
      "source": [
        "# Unir los datasets usando la columna 'country_code'\n",
        "df_merged= pd.merge(gdp_df, country_df, on=\"country_code\", how=\"inner\")\n",
        "\n",
        "df_merged = df_merged.rename(columns={\"value\": \"gdp\"})\n",
        "\n",
        "# Mostrar las filas del dataset combinado\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bono: a√±adir una nueva variable"
      ],
      "metadata": {
        "id": "f_Tpnyg5HHN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tom√≥ la variable de porcentaje de inflaci√≥n anual para 266 pa√≠ses con datos desde 1960 hasta 2022. Esta base de datos se tom√≥ del Banco Mundial y se encuentra en este enlace: https://data.worldbank.org/indicator/FP.CPI.TOTL.ZG?view=chart\n",
        "\n",
        "Se escogi√≥ tomar la variable de inflaci√≥n ya que se cree que niveles altos de inflaci√≥n tiene una relaci√≥n negativa sobre el PIB. La inlaci√≥n indica los aumentos generalizados de precios, lo cual reduce el poder adquisitivo y puede contraer el consumo y la inversi√≥n, componentes cruciales del PIB. As√≠ una inflaci√≥n alta y vol√°til puede desacelerar el crecimiento econ√≥mico, afectando negativamente la competitividad y el comercio exterior de un pa√≠s, lo que se puede asociar con un menor PIB. Por este motivo, consideramos que la variables de inflaci√≥n puede ser √∫til para predecir el nivel del PIB de cada pa√≠s."
      ],
      "metadata": {
        "id": "V6liNiZ5HL62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar la base de datos de inflaci√≥n manualmente\n",
        "df_inflacion = pd.read_csv(\"/content/inflacion_data.csv\", sep=\",\", skiprows=3)\n",
        "\n",
        "df_inflacion.info()\n",
        "\n",
        "df_inflacion = df_inflacion.drop(columns=['Country Name', 'Indicator Name', 'Indicator Code', '2023', 'Unnamed: 68'])\n",
        "\n",
        "df_inflacion"
      ],
      "metadata": {
        "id": "DTZJcTH6H3fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertir los a√±os en filas para que quede en el mismo formato que gdp_df\n",
        "\n",
        "df_inflacion_long = df_inflacion.melt(id_vars=[\"Country Code\"], var_name=\"year\", value_name=\"inflation\")\n",
        "df_inflacion_long[\"year\"] = df_inflacion_long[\"year\"].astype(int)\n",
        "df_inflacion_long = df_inflacion_long.rename(columns={\"Country Code\": \"country_code\"})\n",
        "\n",
        "df_inflacion_long"
      ],
      "metadata": {
        "id": "jaYmnCBFIWMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unir el merge realizado anteriormente con los datos del PIB con la base de datos de inflaci√≥n usando el c√≥digo dle pa√≠s\n",
        "df = df_merged.merge(df_inflacion_long, on=[\"country_code\", \"year\"], how=\"right\")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "dMlN-F7-Im_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlTSp3HDe4Lq"
      },
      "source": [
        "##**Explorar el dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX_9Ft_rfHPo"
      },
      "source": [
        "**Diccionario de variables**\n",
        "\n",
        "| **Variable**   | **Descripci√≥n**                                                                                  | **Tipo de dato** |\n",
        "|---------------|--------------------------------------------------------------------------------------------------|------------------|\n",
        "| country_name  | Nombre del pa√≠s.                                                                                 | Texto           |\n",
        "| country_code  | C√≥digo del pa√≠s seg√∫n la norma (c√≥digo de tres letras).                        | Texto           |\n",
        "| year          | A√±o al que corresponde el dato.                                                          | Num√©rico        |\n",
        "| value         | Valor del PIB en d√≥lares estadounidenses actuales.                                               | Num√©rico        |\n",
        "| region        | Regi√≥n geogr√°fica a la que pertenece el pa√≠s (por ejemplo, Asia, Europa).                        | Texto           |\n",
        "| income_group  | Clasificaci√≥n del pa√≠s seg√∫n su nivel de ingresos (por ejemplo, ingresos altos, medios, bajos). | Texto           |\n",
        "| inflation     | Tasa de inflaci√≥n anual (%), basada en el √≠ndice de precios al consumidor.                        | Num√©rico        |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVR1r_g6TrCr"
      },
      "outputs": [],
      "source": [
        "# Informaci√≥n general de los datasets\n",
        "print(gdp_df.info())\n",
        "print(country_df.info())\n",
        "print(df_inflacion.info())\n",
        "\n",
        "# Estad√≠sticas descriptivas\n",
        "print (\"Estad√≠sticas descriptivas de la base de datos\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20Oteu9QdTR"
      },
      "source": [
        "## **Clasificaci√≥n de variables y preparaci√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "123RLepV68Jd"
      },
      "outputs": [],
      "source": [
        "# Hacer Label Encoder para la variable 'income_group'\n",
        "\n",
        "# Crear un diccionario con los valores personalizados\n",
        "income_mapping = {\n",
        "    \"High income\": 4,\n",
        "    \"Upper middle income\": 3,\n",
        "    \"Lower middle income\": 2,\n",
        "    \"Low income\": 1\n",
        "}\n",
        "\n",
        "# Aplicar el mapeo a la columna 'income_group'\n",
        "df['income_group'] = df['income_group'].map(income_mapping)\n",
        "\n",
        "# Aplicar One-Hot Encoding a 'region'\n",
        "df_encoded = pd.get_dummies(df, columns=['region'])\n",
        "df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBzf5QX-hhcg"
      },
      "outputs": [],
      "source": [
        "# Para transformar la variable categ√≥rica de los pa√≠ses\n",
        "\n",
        "# Modificar el nombre de los paises por un ranking\n",
        "country_gpd = df_encoded.groupby('country_name')['gdp'].mean().sort_values()\n",
        "country_ranking = {country: rank for rank, country in enumerate(country_gpd.index)}\n",
        "\n",
        "df_encoded['country_name'] = df_encoded['country_name'].map(country_ranking)\n",
        "\n",
        "df_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF3Z9sb_HPDF"
      },
      "outputs": [],
      "source": [
        "#Valores √∫nicos en la variable a√±o\n",
        "df_encoded['year'].unique()\n",
        "\n",
        "#Transformaci√≥n de los a√±os en columnas\n",
        "\n",
        "df_final = df.pivot(index=\"country_code\", columns=\"year\", values=[\"gdp\", \"inflation\"])\n",
        "\n",
        "df_final.columns[-64]\n",
        "\n",
        "df_final\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificar valor NA en la base de datos\n",
        "print(df_final.isna().sum())\n",
        "\n",
        "# Reemplazar todos los NaN en el DataFrame con -1\n",
        "df_final.fillna(-1, inplace=True)\n",
        "\n",
        "print(df_final.isna().sum())"
      ],
      "metadata": {
        "id": "Ip-NWWQ_KnIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificaci√≥n de valores nulos\n",
        "print(df_final.isnull().sum())"
      ],
      "metadata": {
        "id": "eXC03zOuL9NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHP4Kcs7_dX1"
      },
      "source": [
        "**Crear la variable objetivo (GDP para el a√±o 2022)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.columns[-64]\n"
      ],
      "metadata": {
        "id": "Nlr7-YP0MV0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2AuLUR0W1RP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Definir el nombre de la columna correspondiente al PIB del 2022\n",
        "gdp_2022 = ('gdp', 2022)\n",
        "gdp_2021 = ('gdp', 2021)\n",
        "\n",
        "# Reemplazar NaN en la columna 2022 con los valores de la columna 2021\n",
        "df_final[gdp_2022] = df_final[gdp_2022].fillna(df_final[gdp_2021])\n",
        "\n",
        "# Calcular percentiles SOLO sobre la columna 2022\n",
        "low_threshold = np.percentile(df_final[gdp_2022].dropna(), 33)  # 33% de los datos\n",
        "high_threshold = np.percentile(df_final[gdp_2022].dropna(), 66)  # 66% de los datos\n",
        "\n",
        "# Funci√≥n para asignar la categor√≠a basada en los percentiles de 2022\n",
        "def categorize_gdp(value):\n",
        "    if value <= low_threshold:\n",
        "        return \"Low GDP\"\n",
        "    elif value <= high_threshold:\n",
        "        return \"Medium GDP\"\n",
        "    else:\n",
        "        return \"High GDP\"\n",
        "\n",
        "# Aplicar la funci√≥n de categorizaci√≥n sobre la columna GDP_2022\n",
        "df_final[\"GDP_Level\"] = df_final[gdp_2022].apply(categorize_gdp)\n",
        "\n",
        "# Mostrar la distribuci√≥n de clases\n",
        "print(df_final[\"GDP_Level\"].value_counts())\n",
        "\n",
        "# Codificar la variable categ√≥rica con LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_final[\"GDP_Level\"] = le.fit_transform(df_final[\"GDP_Level\"])\n",
        "\n",
        "df_final.drop(columns=[gdp_2022], inplace=True)\n",
        "\n",
        "# Mostrar primeras filas con la variable de clasificaci√≥n\n",
        "display(df_final.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnOCA0QC24G1"
      },
      "source": [
        "## **Divisi√≥n de los datos en test y train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qejn75iZgxx"
      },
      "outputs": [],
      "source": [
        "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
        "X = df_final.drop(columns=[\"GDP_Level\"])\n",
        "y = df_final[\"GDP_Level\"]\n",
        "\n",
        "# Dividir en 80% entrenamiento y 20% prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Mostrar dimensiones de los conjuntos\n",
        "print(f\"Tama√±o del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Tama√±o del conjunto de prueba: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_wfMiW649B-"
      },
      "outputs": [],
      "source": [
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEbNjdiL3BwM"
      },
      "source": [
        "### **An√°lisis descriptivo de los datos de train y test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cgv1_p-3HR-"
      },
      "source": [
        "#### **An√°lisis datos train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqukum-D3ccD"
      },
      "outputs": [],
      "source": [
        "# Reporte an√°lisis descriptivo de los datos de entrenamiento\n",
        "reporte_train = ProfileReport(df_train, title=\"Profiling Report Train dataset\", minimal=True)\n",
        "reporte_train.to_file(\"reporte_train.html\")\n",
        "reporte_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEN9PUQO5ctE"
      },
      "source": [
        "#### **An√°lisis datos test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQlAwRm65j8x"
      },
      "outputs": [],
      "source": [
        "# Reporte an√°lisis descriptivo de los datos de entrenamiento\n",
        "reporte_test = ProfileReport(df_train, title=\"Profiling Report Test dataset\", minimal=True)\n",
        "reporte_test.to_file(\"reporte_test.html\")\n",
        "reporte_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p8arN81ZlU5"
      },
      "source": [
        "# **2. Construcci√≥n modelos redes neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. a. Modelo de red neuronal tradicional (Scikit-Learn)"
      ],
      "metadata": {
        "id": "s3wCXBBINlAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmilR9gNZxko"
      },
      "outputs": [],
      "source": [
        "#Estandarizar los datos de X de la base detos\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# üìå Guardar nombres de columnas y el √≠ndice antes de escalar\n",
        "column_names = X_train.columns\n",
        "index_train = X_train.index\n",
        "index_test = X_test.index\n",
        "\n",
        "# üìå Escalar los datos despu√©s de la limpieza\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# üìå Convertir de nuevo a DataFrame manteniendo nombres de columnas e √≠ndice\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=column_names, index=index_train)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=column_names, index=index_test)\n",
        "\n",
        "display(X_train_scaled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B√∫squeda de hiperpar√°metros para modelo tradicional"
      ],
      "metadata": {
        "id": "g2ZusNjTOCF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Definir los hiperpar√°metros a optimizar con solo una capa oculta\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(16,), (32,), (64,), (128,)],  # Diferentes tama√±os para una √∫nica capa oculta\n",
        "    'activation': ['relu', 'tanh', 'logistic'],  # Funci√≥n de activaci√≥n\n",
        "    'solver': ['adam', 'lbfgs'],  # M√©todos de optimizaci√≥n\n",
        "    'alpha': [0.0001, 0.01, 0.1],  # Regularizaci√≥n L2\n",
        "    'learning_rate_init': [0.001, 0.01, 0.05],  # Tasa de aprendizaje inicial\n",
        "    'batch_size': [32, 64, 'auto'],  # Tama√±o del lote\n",
        "}\n",
        "\n",
        "# Definir el modelo base\n",
        "mlp = MLPClassifier(max_iter=200, random_state=42)\n",
        "\n",
        "# Configurar Grid Search\n",
        "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Ejecutar la b√∫squeda de hiperpar√°metros\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Mostrar los mejores hiperpar√°metros encontrados\n",
        "print(\"\\nüîç Mejores hiperpar√°metros encontrados:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Entrenar modelo con los mejores hiperpar√°metros\n",
        "best_mlp = grid_search.best_estimator_\n",
        "\n",
        "# Evaluar en el conjunto de prueba\n",
        "y_pred = best_mlp.predict(X_test_scaled)\n",
        "print(\"\\n‚úÖ Resultados del mejor modelo:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PacFUhJNeTXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K8Tw3jPaTAc"
      },
      "outputs": [],
      "source": [
        "#  PARA TEST\n",
        "#Paso 1: Hacer predicciones\n",
        "y_pred = best_mlp.predict(X_test_scaled)\n",
        "\n",
        "#  Paso 2: Imprimir reporte de clasificaci√≥n\n",
        "print(\"\\n‚úÖ Reporte de Clasificaci√≥n:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#  Paso 3: Graficar la Matriz de Confusi√≥n\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusi√≥n - MLPClassifier Test\")\n",
        "plt.show()\n",
        "\n",
        "#  PARA TRAIN\n",
        "#Paso 1: Hacer predicciones\n",
        "y_pred = best_mlp.predict(X_train_scaled)\n",
        "\n",
        "#  Paso 2: Imprimir reporte de clasificaci√≥n\n",
        "print(\"\\n‚úÖ Reporte de Clasificaci√≥n:\")\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "#  Paso 3: Graficar la Matriz de Confusi√≥n\n",
        "conf_matrix = confusion_matrix(y_train, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusi√≥n - MLPClassifier Train\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gr√°fica funci√≥n de p√©rdida por iteraci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üìå Verificar si el modelo ha almacenado el historial de p√©rdidas\n",
        "if hasattr(best_mlp, 'loss_curve_'):\n",
        "    plt.plot(best_mlp.loss_curve_, label='Loss durante entrenamiento')\n",
        "    plt.xlabel('Iteraciones (√âpocas)')\n",
        "    plt.ylabel('P√©rdida (Loss)')\n",
        "    plt.title('Evoluci√≥n de la P√©rdida durante el Entrenamiento')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"El modelo no ha almacenado la curva de p√©rdida.\")\n",
        "\n",
        "#Gr√°fica tasa de aprendizaje\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üìå Definir la tasa de aprendizaje inicial (se obtiene de los hiperpar√°metros del mejor modelo)\n",
        "learning_rate_init = best_mlp.learning_rate_init\n",
        "\n",
        "# üìå Simular la evoluci√≥n de la tasa de aprendizaje si es 'adaptive'\n",
        "if best_mlp.learning_rate == 'adaptive':\n",
        "    learning_rates = [learning_rate_init / (1.0 + 0.1 * i) for i in range(len(best_mlp.loss_curve_))]\n",
        "else:\n",
        "    learning_rates = [learning_rate_init] * len(best_mlp.loss_curve_)  # Tasa de aprendizaje fija\n",
        "\n",
        "# Graficar la tasa de aprendizaje\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(learning_rates, label=\"Tasa de Aprendizaje\", color='red')\n",
        "plt.xlabel('Iteraciones (√âpocas)')\n",
        "plt.ylabel('Tasa de Aprendizaje')\n",
        "plt.title('Evoluci√≥n de la Tasa de Aprendizaje durante el Entrenamiento')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#Curva ROC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# üìå Binarizar las clases de y_test para el c√°lculo de la ROC multiclase\n",
        "n_classes = len(np.unique(y_test))  # N√∫mero de clases √∫nicas\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))  # Convierte en formato binarizado\n",
        "y_probs = best_mlp.predict_proba(X_test_scaled)  # Probabilidades de cada clase\n",
        "\n",
        "# üìå Calcular ROC para cada clase\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_probs[:, i])  # ROC para cada clase\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'Clase {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "# üìå Agregar l√≠nea de referencia y etiquetas\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC Multiclase')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Gr√°fica de precisi√≥n"
      ],
      "metadata": {
        "id": "qNVi-XOOTa-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gr√°fica precisi√≥n del modelo"
      ],
      "metadata": {
        "id": "1z9sihd7W0Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üìå Verificar si el modelo tiene historial de entrenamiento\n",
        "if hasattr(best_mlp, \"loss_curve_\"):\n",
        "    plt.plot(best_mlp.loss_curve_, label=\"P√©rdida en entrenamiento\", color=\"b\")\n",
        "\n",
        "    plt.xlabel(\"Iteraciones (√âpocas)\")\n",
        "    plt.ylabel(\"P√©rdida (Loss)\")\n",
        "    plt.title(\"Evoluci√≥n de la P√©rdida del Modelo\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay historial de p√©rdida disponible. Verifica que el modelo haya sido entrenado correctamente.\")\n"
      ],
      "metadata": {
        "id": "3ccp2-XbWcZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para guardar el modelo\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Guardar modelo de Scikit-Learn\n",
        "joblib.dump(best_mlp, \"modelo_sklearn.pkl\")\n",
        "\n",
        "# Cargar modelo despu√©s\n",
        "modelo_sklearn = joblib.load(\"modelo_sklearn.pkl\")\n"
      ],
      "metadata": {
        "id": "NAJkrhXNclEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. b. Modelo red neuronal profunda (Tensorflow)"
      ],
      "metadata": {
        "id": "3akexs-lhu5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6wj7yG3auSH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# üìå Paso 1: Definir la arquitectura de la Red Neuronal Profunda\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Capa de entrada\n",
        "    Dropout(0.2),  # Regularizaci√≥n\n",
        "    Dense(64, activation='relu'),  # Primera capa oculta\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),  # Segunda capa oculta\n",
        "    Dense(3, activation='softmax')  # Capa de salida (3 clases: Low, Medium, High GDP)\n",
        "])\n",
        "\n",
        "# üìå Paso 2: Compilar el modelo\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),  # Optimizador Adam\n",
        "              loss='sparse_categorical_crossentropy',  # Funci√≥n de p√©rdida\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# üìå Paso 3: Entrenar el modelo\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# üìå Paso 4: Evaluaci√≥n final\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"\\nüìä Precisi√≥n en el conjunto de prueba: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meTWY9R6a9hU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#PARA TEST\n",
        "# üìå Hacer predicciones\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "y_pred_nn = y_pred_nn.argmax(axis=1)  # Convertir probabilidades en clases\n",
        "\n",
        "# üìå Imprimir reporte de clasificaci√≥n\n",
        "print(\"\\n‚úÖ Reporte de Clasificaci√≥n - Red Neuronal TensorFlow:\")\n",
        "print(classification_report(y_test, y_pred_nn))\n",
        "\n",
        "# üìå Matriz de Confusi√≥n\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_nn)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusi√≥n - TensorFlow Test\")\n",
        "plt.show()\n",
        "\n",
        "#PARA TRAIN\n",
        "# üìå Hacer predicciones\n",
        "y_pred_nn = model.predict(X_train_scaled)\n",
        "y_pred_nn = y_pred_nn.argmax(axis=1)  # Convertir probabilidades en clases\n",
        "\n",
        "# üìå Imprimir reporte de clasificaci√≥n\n",
        "print(\"\\n‚úÖ Reporte de Clasificaci√≥n - Red Neuronal TensorFlow:\")\n",
        "print(classification_report(y_train, y_pred_nn))\n",
        "\n",
        "# üìå Matriz de Confusi√≥n\n",
        "conf_matrix = confusion_matrix(y_train, y_pred_nn)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"Matriz de Confusi√≥n - TensorFlow Train\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXmXWzMCa5bT"
      },
      "outputs": [],
      "source": [
        "#Gr√°fica tasa de aprendizaje\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extraer la tasa de aprendizaje de cada √©poca desde el optimizador\n",
        "learning_rates = [model.optimizer.learning_rate.numpy() for _ in history.epoch]\n",
        "\n",
        "# Graficar la evoluci√≥n de la tasa de aprendizaje\n",
        "plt.plot(history.epoch, learning_rates, label=\"Tasa de Aprendizaje\")\n",
        "plt.xlabel(\"√âpocas\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Evoluci√≥n de la Tasa de Aprendizaje\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# P√©rdida del modelo\n",
        "plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Validaci√≥n')\n",
        "plt.title(\"Evoluci√≥n de la funci√≥n de p√©rdida modelo redes neuronales profundas\")\n",
        "plt.xlabel(\"√âpocas\")\n",
        "plt.ylabel(\"P√©rdida\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Precisi√≥n en entrenamiento y validaci√≥n\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Validaci√≥n')\n",
        "plt.title(\"Evoluci√≥n de la precisi√≥n modelo redes neuronales profundas\")\n",
        "plt.xlabel(\"√âpocas\")\n",
        "plt.ylabel(\"Precisi√≥n\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Suponiendo que entrenaste el modelo con history = modelo.fit(...)\n",
        "plt.plot(history.history['loss'], label='P√©rdida en entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='P√©rdida en validaci√≥n')\n",
        "\n",
        "plt.xlabel('Iteraciones (√âpocas)')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.title('Tasa de Aprendizaje vs Iteraciones')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Para guardar el modelo\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# Guardar modelo de TensorFlow\n",
        "modelo_tf.save(\"modelo_tensorflow.keras\")\n",
        "\n",
        "# Cargar modelo despu√©s\n",
        "modelo_tensorflow = keras.models.load_model(\"modelo_tensorflow.keras\")"
      ],
      "metadata": {
        "id": "C-4KX9Hycvpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Red neuronal con mala funci√≥n de p√©rdida"
      ],
      "metadata": {
        "id": "iBiggcXZiCeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j971aTGBbIUP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# üìå Paso 1: Definir la arquitectura del modelo con errores\n",
        "model_bad = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Capa de entrada\n",
        "    Dense(64, activation='relu'),  # Primera capa oculta (sin Dropout)\n",
        "    Dense(32, activation='relu'),  # Segunda capa oculta (sin BatchNorm)\n",
        "    Dense(3, activation='softmax')  # Capa de salida (3 clases)\n",
        "])\n",
        "\n",
        "\n",
        "# üìå Paso 2: Compilar con errores\n",
        "model_bad.compile(optimizer=SGD(learning_rate=1.0),  # Learning rate demasiado alto\n",
        "                  loss='sparse_categorical_crossentropy',  # Funci√≥n de p√©rdida incorrecta (deber√≠a ser 'sparse_categorical_crossentropy')\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# üìå Paso 3: Entrenar el modelo\n",
        "history_bad = model_bad.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# üìå Paso 4: Evaluaci√≥n del modelo\n",
        "test_loss_bad, test_acc_bad = model_bad.evaluate(X_test_scaled, y_test)\n",
        "print(f\"\\n‚ùå Precisi√≥n en test (modelo err√≥neo): {test_acc_bad:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INJNhHsLbQVI"
      },
      "outputs": [],
      "source": [
        "# üìå Gr√°fica de la p√©rdida (se espera que sea inestable)\n",
        "plt.plot(history_bad.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history_bad.history['val_loss'], label='Validaci√≥n')\n",
        "plt.title(\"üö® P√©rdida en el modelo mal configurado\")\n",
        "plt.xlabel(\"√âpocas\")\n",
        "plt.ylabel(\"P√©rdida\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# üìå Gr√°fica de la precisi√≥n (se espera que no mejore mucho)\n",
        "plt.plot(history_bad.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history_bad.history['val_accuracy'], label='Validaci√≥n')\n",
        "plt.title(\"üö® Precisi√≥n en el modelo mal configurado\")\n",
        "plt.xlabel(\"√âpocas\")\n",
        "plt.ylabel(\"Precisi√≥n\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N7j-iDUbSJ9"
      },
      "outputs": [],
      "source": [
        "# üìå Hacer predicciones con el modelo mal configurado\n",
        "y_pred_bad = model_bad.predict(X_test_scaled)\n",
        "y_pred_bad = y_pred_bad.argmax(axis=1)  # Convertir probabilidades en clases\n",
        "\n",
        "# üìå Matriz de confusi√≥n\n",
        "conf_matrix_bad = confusion_matrix(y_test, y_pred_bad)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(conf_matrix_bad, annot=True, fmt='d', cmap='Reds', xticklabels=[\"Low\", \"Medium\", \"High\"], yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores Reales\")\n",
        "plt.title(\"üö® Matriz de Confusi√≥n - Modelo Mal Configurado\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}